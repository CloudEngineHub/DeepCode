from __future__ import annotations

import re

from mcp_agent.config import get_settings
from mcp_agent.core.context import initialize_context
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

"""OrchestratorAgent — dynamic DAG executor for mcp‑agent workflow plans
=======================================================================
This agent receives a JSON *agent_orchestration_plan* generated by
RouterAgent, builds a DAG, and executes each step by instantiating the
corresponding specialised `mcp_agent.agents.agent.Agent` subclass.

Key features
------------
* **Dynamic DAG** – supports any topologically‑valid plan with arbitrary
  fan‑out / fan‑in.
* **Artifact Store** – `self.artifacts` dict lets steps share objects in
  memory; each artifact is also checkpoint‑persisted to `workspace/` so
  the run can be resumed after failure.
* **Missing‑input handling** – if a step is flagged
  `requires_user_input: true`, execution pauses and emits
  `UserInputRequired` for external UI to collect the data.
* **Retry / degrade** – each step retries up to `max_retries` times.  If
  still failing, behaviour is configurable (`skip` | `error`).

The class works with the standard *mcp‑agent* API: we rely on
`Agent(...).attach_llm()` and `Agent.list_tools()` under the hood.
"""

from dataclasses import dataclass, field
from pathlib import Path
import asyncio
import json
import logging
import networkx as nx
from typing import Any, Dict, List, Optional, Type

from mcp_agent.agents.agent import Agent  # base class from your codebase
from utils.json_tools import safe_json_loads  # robust JSON parser used elsewhere

logger = logging.getLogger(__name__)

DEFAULT_WORKSPACE_ROOT = Path("deepcode_lab")
def _persist(self, art_key: str, value: Any):
    fname = self.workspace / f"{art_key}.json"
    try:
        with fname.open("w", encoding="utf-8") as f:
            json.dump(value, f, ensure_ascii=False, indent=2)
    except (TypeError, PermissionError, OSError) as e:
        logger.error("💾 Persist error for %s → %s: %s", art_key, fname, e)
        raise                    # 让上层重试/降级
def safe_dirname(raw: str, max_len: int = 15) -> str:
    """
    将任意字符串裁剪成磁盘安全的目录名：
        1. 把非字母数字替换成下划线
        2. 去掉首尾下划线
        3. 截取前 max_len 个字符
    """
    slug = re.sub(r"[^A-Za-z0-9]+", "_", raw).strip("_")
    return slug[:max_len]

@dataclass
class StepSpec:
    id: str
    agent_name: str
    inputs: Dict[str, Any]
    outputs: List[str]
    depends_on: List[str] = field(default_factory=list)
    server_names: List[str] = field(default_factory=list)
    requires_user_input: bool = False
    max_retries: int = 3
    degrade_strategy: str = "error"  # or "skip"

    @staticmethod
    def from_json(obj: Dict[str, Any]) -> "StepSpec":
        return StepSpec(
            id=obj["id"],
            agent_name=obj["agent"],
            inputs=obj.get("inputs", {}),
            outputs=obj.get("outputs", []),
            depends_on=obj.get("depends_on", []),
            server_names=obj.get("server_names", []),
            requires_user_input=obj.get("requires_user_input", False),
        )
project_root = Path(__file__).resolve().parents[2]  # DeepCode-main
config_path  = project_root / "mcp_agent.config.yaml"
secrets_path = project_root / "mcp_agent.secrets.yaml"
class OrchestratorAgent:
    """Runs a plan JSON using existing specialised Agents."""

    def __init__(
        self,
        plan_json: str | Dict[str, Any],
        registry: Dict[str, Type[Agent]],
        workspace: Path | str | None = None,
    ) -> None:
        # Parse plan
        if isinstance(plan_json, str):
            plan = safe_json_loads(plan_json)
        else:
            plan = plan_json

        if plan.get("plan_type") != "agent_orchestration_plan":
            raise ValueError("unsupported plan_type")

        self.global_context = plan.get("global_context", {})
        self.steps: Dict[str, StepSpec] = {
            s["id"]: StepSpec.from_json(s) for s in plan["steps"]
        }
        self.registry = registry
        self.artifacts: Dict[str, Any] = {}
        self.llm = OpenAIAugmentedLLM(
            model="deepseek-chat",
            config_path=secrets_path
        )
        self.workspace = Path(workspace) if workspace else DEFAULT_WORKSPACE_ROOT / Path(
            safe_dirname(self.global_context.get("goal", "generic_task"))
        )
        self.workspace.mkdir(parents=True, exist_ok=True)

        self._build_graph()

        self._config_path = "mcp_agent.config.yaml"
        self._context = None
            # ------------------------------------------------------------------
    # DAG helpers
    # ------------------------------------------------------------------
    def _build_graph(self):
        self.G = nx.DiGraph()
        for sid, spec in self.steps.items():
            self.G.add_node(sid)
            for dep in spec.depends_on:
                self.G.add_edge(dep, sid)
        if not nx.is_directed_acyclic_graph(self.G):
            raise ValueError("Plan has cyclic dependencies")

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def run_async(self):

        # 在这里初始化 Context（拿到 settings 后）
        if self._context is None:
            self._context = await initialize_context(
                config=get_settings(self._config_path),
                store_globally=True  # 或 False，二选一；有全局读取需求可设 True
                )

            # 准备一个 llm_factory（以 DeepSeek OpenAI 兼容为例）
        self._llm_factory = lambda agent: OpenAIAugmentedLLM(
            agent=agent,
            context=self._context,
            model="deepseek-chat"
        )
        #"""Execute the DAG asynchronously."""
        # queue of ready step ids
        ready = [n for n in self.G.nodes if self.G.in_degree(n) == 0]
        in_progress: set[str] = set()
        completed: set[str] = set()

        while ready or in_progress:
            # schedule ready steps in parallel
            tasks = {}
            for sid in ready:
                in_progress.add(sid)
                tasks[sid] = asyncio.create_task(self._run_step(self.steps[sid]))
            ready.clear()

            # wait for first finished task
            done, _ = await asyncio.wait(tasks.values(), return_when=asyncio.FIRST_COMPLETED)
            for t in done:
                sid, success = t.result()  # each task returns (sid, success)
                in_progress.discard(sid)
                if success:
                    completed.add(sid)
                    # push children whose deps are all completed
                    for child in self.G.successors(sid):
                        if child in completed or child in in_progress:
                            continue
                        if all(pred in completed for pred in self.steps[child].depends_on):
                            ready.append(child)
                else:
                    logger.error("Step %s failed irrecoverably", sid)
                    if self.steps[sid].degrade_strategy == "error":
                        raise RuntimeError(f"Workflow halted due to step {sid} failure")
                    # skip strategy: mark as completed with None outputs
                    completed.add(sid)

        logger.info("Workflow completed ✅")

    # ------------------------------------------------------------------
    # Step executor
    # ------------------------------------------------------------------
    async def _run_step(self, spec: StepSpec) -> tuple[str, bool]:
        """Execute a single step. Return (step_id, success_flag)."""
        logger.info("▶️  Step %s (%s)", spec.id, spec.agent_name)
        entry = self.registry.get(spec.agent_name)
        if entry is None:
            logger.error("Unknown agent: %s", spec.agent_name)
            return spec.id, False
        elif isinstance(entry,Agent):
            agent_cls =entry
        elif isinstance(entry, type) and issubclass(entry, Agent):
            async with entry(
                    name=spec.id,
                    instruction="",
                    server_names=spec.server_names,
            ) as agent_cls:
                pass# Resolve inputs
        try:
            inputs = {k: self._resolve(v) for k, v in spec.inputs.items()}
        except KeyError as e:
            if spec.requires_user_input:
                logger.warning("Step %s requires user input: %s", spec.id, e)
                raise UserInputRequired(f"Step {spec.id} missing input {e}")
            else:
                logger.error("Missing input %s for step %s", e, spec.id)
                return spec.id, False

        # retry loop
        for attempt in range(1, spec.max_retries + 1):
            try:
                #这里出错
                llm = await agent_cls.attach_llm(llm_factory=self._llm_factory)

                # 1) 组装 messages（优先用你 step 传来的 messages；否则用 prompt；再否则把 inputs 串成一段）
                if "messages" in inputs and isinstance(inputs["messages"], list):
                    messages = inputs["messages"]
                else:
                    user_text = inputs.get("prompt") or json.dumps(inputs, ensure_ascii=False)
                    messages = [
                        {"role": "system",
                         "content": getattr(agent_cls, "instruction", "") or "You are a helpful agent."},
                        {"role": "user", "content": user_text},
                    ]

                # 2) 调 LLM
                out_messages = await llm.generate(messages)
                # 3) 把模型输出转成纯文本（兼容 pydantic/dict 两种形态）
                texts = []
                for m in out_messages:
                    role = getattr(m, "role", None) if not isinstance(m, dict) else m.get("role")
                    if role == "assistant":
                        content = getattr(m, "content", None) if not isinstance(m, dict) else m.get("content")
                        if isinstance(content, str):
                            texts.append(content)
                        elif isinstance(content, list):
                            parts = []
                            for c in content:
                                typ = getattr(c, "type", None) if not isinstance(c, dict) else c.get("type")
                                if typ == "text":
                                    parts.append(
                                        getattr(c, "text", "") if not isinstance(c, dict) else c.get("text", ""))
                            texts.append("\n".join(p for p in parts if p))
                        else:
                            # 兜底
                            try:
                                texts.append(m.model_dump().get("content", ""))  # pydantic v2
                            except Exception:
                                texts.append(str(content))

                # texts 就是这一步的“结果文本”
                result = "\n".join(t for t in texts if t)

                if not isinstance(result, (list, tuple)):
                    result = [result]
                # map outputs
                for name, val in zip(spec.outputs, result):
                    art_key = f"{spec.id}.{name}"
                    self.artifacts[art_key] = val
                    self._persist(art_key, val)
                logger.info("✅  Step %s succeeded", spec.id)
                return spec.id, True
            except Exception as e:
                logger.warning("Attempt %d/%d failed for step %s: %s", attempt, spec.max_retries, spec.id, e)
                await asyncio.sleep(2 ** (attempt - 1))
        logger.error("❌  Step %s failed after %d retries", spec.id, spec.max_retries)
        return spec.id, False

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _resolve(self, val):
        """Resolve artifact reference like 'step.output' into actual object."""
        if isinstance(val, str) and val.count(".") == 1:
            step_id, key = val.split(".")
            art_key = f"{step_id}.{key}"
            if art_key in self.artifacts:
                return self.artifacts[art_key]
        return val

    def _persist(self, art_key: str, value: Any):
        """Persist artifact as JSON if possible, fallback to pickle."""
        fname = self.workspace / f"{art_key}.json"
        try:
            with fname.open("w", encoding="utf-8") as f:
                json.dump(value, f, ensure_ascii=False, indent=2)
        except TypeError:
            import pickle, gzip
            fname = self.workspace / f"{art_key}.pkl.gz"
            with gzip.open(fname, "wb") as f:
                pickle.dump(value, f)

# ----------------- Example Usage ------------------------------------
if __name__ == "__main__":
    import asyncio
    from workflows.agents.registry import registry  # your code should provide this mapping
    #需要注册表
    with open("D:\PythonProjects\DeepCode-main/1.txt", "r", encoding="utf-8") as f:
        PLAN_JSON = json.load(f)

    orch = OrchestratorAgent(PLAN_JSON, registry)
    asyncio.run(orch.run_async())
