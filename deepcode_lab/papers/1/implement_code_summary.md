# Code Implementation Progress Summary
*Accumulated implementation progress for all files*


================================================================================
## IMPLEMENTATION File src/rice/explanation/mask_network.py; ROUND 3 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 16:57:10
**File Implemented**: src/rice/explanation/mask_network.py

**Core Purpose**:
- The `mask_network.py` file implements the `StateMask` class, which generates binary masks for state components in reinforcement learning environments. This is used to identify critical state dimensions for explanation and refinement in the RICE system.

**Public Interface**:
- Class `StateMask`: Produces binary masks for state components to identify critical dimensions.
  - Key methods: 
    - `__init__(self, state_dim, hidden_sizes=[256, 256], activation=nn.ReLU)`: Initializes the network with specified dimensions and activation functions.
    - `forward(self, states)`: Computes mask logits, probabilities, and binary actions from input states.
    - `compute_explanation_loss(self, states, actions, rewards, alpha=0.0001)`: Computes the explanation loss incorporating a blinding bonus.
    - `get_critical_states(self, states, threshold=0.8)`: Identifies critical states based on mask probabilities.
  - Constructor params: `state_dim`, `hidden_sizes`, `activation`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn`, `torch.nn.functional`
- External packages: `numpy` - used for numerical operations

**External Dependencies**:
- Expected to be imported by: `src/rice/explanation/training.py`, `src/rice/core/rice.py`
- Key exports used elsewhere: `StateMask` class

**Implementation Notes**:
- Architecture decisions: Utilizes fully connected layers with ReLU activation to process input states and produce mask logits. Implements Gumbel-Sigmoid for differentiable sampling during training.
- Cross-File Relationships: Likely interacts with training and core RICE system files to integrate mask generation and explanation loss into the reinforcement learning pipeline.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/explanation/training.py; ROUND 5 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 16:57:57
**File Implemented**: src/rice/explanation/training.py

**Core Purpose**:
- The `src/rice/explanation/training.py` file is responsible for training the StateMask network using a PPO-style optimization approach. It manages the training loop, computes advantages, and updates the network parameters to improve the mask generation for reinforcement learning tasks.

**Public Interface**:
- Class `MaskTrainer`: Manages the training of the StateMask network.
  - Key methods: `__init__`, `store_transition`, `compute_gae`, `update`, `get_mask`, `save`, `load`
  - Constructor params: `state_dim`, `hidden_sizes`, `lr`, `batch_size`, `ppo_epochs`, `clip_param`, `value_loss_coef`, `entropy_coef`, `max_grad_norm`, `blinding_bonus_coef`, `device`

**Internal Dependencies**:
- From `src/rice/explanation/mask_network`: `StateMask`
- External packages:
  - `torch` - Used for neural network operations and tensor computations.
  - `torch.nn` - Provides neural network layers and modules.
  - `torch.optim` - Used for optimization algorithms.
  - `numpy` - Utilized for numerical operations.
  - `collections.deque` - Used for managing the experience buffer.

**External Dependencies**:
- Expected to be imported by: Likely to be used by files managing the overall training process or integrating the mask network into broader reinforcement learning pipelines.
- Key exports used elsewhere: `MaskTrainer` class for training and managing the StateMask network.

**Implementation Notes**:
- Architecture decisions: Utilizes PPO-style optimization with a focus on mask generation for reinforcement learning. The implementation includes a value network for advantage estimation and a mechanism for storing and processing experience transitions.
- Cross-File Relationships: Works closely with `mask_network.py` to utilize the StateMask architecture for training.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/explanation/objectives.py; ROUND 7 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 16:58:32
**File Implemented**: src/rice/explanation/objectives.py

**Core Purpose**: 
- The `src/rice/explanation/objectives.py` file is responsible for implementing the modified explanation objectives within the RICE framework. It handles various loss computations and objective functions necessary for training the explanation mechanism, focusing on enhancing exploration and policy refinement.

**Public Interface**:
- Class `ExplanationObjectives`: Manages explanation objectives and loss computations for training the explanation mechanism.
  - Key methods: 
    - `__init__(alpha: float, entropy_coef: float, value_loss_coef: float)`: Initializes the class with hyperparameters.
    - `compute_explanation_loss(mask_probs: torch.Tensor, mask_actions: torch.Tensor, rewards: torch.Tensor) -> torch.Tensor`: Computes explanation loss with blinding bonus.
    - `compute_policy_loss(advantages: torch.Tensor, ratio: torch.Tensor, clip_param: float = 0.2) -> torch.Tensor`: Calculates clipped PPO policy loss.
    - `compute_value_loss(values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor`: Computes value function loss.
    - `compute_entropy_loss(mask_probs: torch.Tensor) -> torch.Tensor`: Computes entropy loss to encourage exploration.
    - `compute_total_loss(mask_probs: torch.Tensor, mask_actions: torch.Tensor, rewards: torch.Tensor, values: torch.Tensor, returns: torch.Tensor, ratio: torch.Tensor, advantages: torch.Tensor) -> Tuple[torch.Tensor, dict]`: Computes total combined loss for training.
    - `normalize_advantages(advantages: torch.Tensor) -> torch.Tensor`: Normalizes advantages for stable training.

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn.functional` - Used for tensor operations and loss calculations.
- External packages: `numpy` - General numerical operations.

**External Dependencies**:
- Expected to be imported by: Files handling training and policy refinement, such as `src/rice/explanation/training.py`.
- Key exports used elsewhere: Loss computation methods for training and evaluation processes.

**Implementation Notes**:
- Architecture decisions: The file integrates various loss components (explanation, policy, value, entropy) to form a comprehensive training objective, emphasizing exploration and policy refinement.
- Cross-File Relationships: Works closely with mask network training and policy refinement components to ensure effective explanation and exploration within the RICE framework.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/distribution/mixer.py; ROUND 9 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 16:59:15
**File Implemented**: src/rice/distribution/mixer.py

**Core Purpose**:
- The `src/rice/distribution/mixer.py` file implements the `StateMixer` class, which is responsible for creating a mixed state distribution by combining regular environment states with critical states identified through an explanation mechanism. This is part of the RICE system's approach to enhance reinforcement learning by integrating critical state information.

**Public Interface**:
- **Class `StateMixer`**: Manages the mixing of regular and critical states.
  - **Constructor params**: `beta: float`, `buffer_size: int`, `device: str`
  - **Key methods**:
    - `add_critical_state(state: torch.Tensor, importance_score: float) -> None`: Adds a single critical state with its importance score.
    - `add_critical_states_batch(states: torch.Tensor, importance_scores: torch.Tensor) -> None`: Adds a batch of critical states with their importance scores.
    - `sample_mixed_states(env_states: torch.Tensor, batch_size: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]`: Samples a mixed batch of states, returning both the states and mixing indicators.
    - `get_buffer_stats() -> dict`: Returns statistics about the critical states buffer.
    - `clear_buffer() -> None`: Clears the critical states buffer.

**Internal Dependencies**:
- From `torch`: `torch` (used for tensor operations and device management)
- From `numpy`: `np` (used for numerical operations)
- From `collections`: `deque` (used for buffer management)

**External Dependencies**:
- Expected to be imported by: `src/rice/explanation/mask_network.py`, `src/rice/explanation/training.py`, `src/rice/explanation/objectives.py` (or any file that requires state distribution mixing)
- Key exports used elsewhere: `StateMixer` class and its methods

**Implementation Notes**:
- **Architecture decisions**: The `StateMixer` class uses a deque to manage a buffer of critical states, allowing efficient addition and sampling. The mixing probability (`beta`) controls the ratio of critical to regular states in the mixed distribution.
- **Cross-File Relationships**: This file likely interacts with explanation components that identify critical states and with training components that utilize mixed state distributions for reinforcement learning.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/distribution/sampler.py; ROUND 11 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 16:59:59
**File Implemented**: src/rice/distribution/sampler.py

**Core Purpose**:
- The `sampler.py` file is responsible for implementing the critical state sampling logic within the RICE framework. It identifies and stores critical states based on importance scores and provides efficient sampling of these states for reinforcement learning.

**Public Interface**:
- **Class `CriticalStateSampler`**: Manages the identification, storage, and sampling of critical states.
  - **Key methods**:
    - `identify_critical_states(states, importance_scores)`: Identifies critical states based on importance scores.
    - `update_critical_states(states, importance_scores)`: Updates the internal storage with new critical states.
    - `sample_states(batch_size=None, with_replacement=True)`: Samples states from the stored critical states.
    - `get_stats()`: Retrieves statistics about the current critical state storage.
    - `clear()`: Clears all stored critical states.
  - **Constructor params**: `state_dim`, `importance_threshold`, `min_samples`, `max_samples`, `device`

**Internal Dependencies**:
- From `src/rice/distribution/mixer.py`: `StateMixer`
- External packages:
  - `torch` - Used for tensor operations and device management.
  - `numpy` - Used for numerical operations, specifically for random sampling.

**External Dependencies**:
- Expected to be imported by: Other components within the RICE framework that require critical state sampling, potentially including training and policy refinement modules.
- Key exports used elsewhere: The `CriticalStateSampler` class and its methods for managing critical states.

**Implementation Notes**:
- **Architecture decisions**: The class uses a list-based storage system for states and importance scores, with mechanisms to maintain a maximum storage size by removing the least important states.
- **Cross-File Relationships**: Works in conjunction with `StateMixer` to provide a comprehensive state sampling mechanism, likely interacting with other components that require prioritized state sampling.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/normalizer.py; ROUND 13 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:00:33
**File Implemented**: src/rice/exploration/normalizer.py

**Core Purpose**:
- The `src/rice/exploration/normalizer.py` file implements a `RunningNormalizer` class designed to maintain stable exploration incentives by normalizing RND exploration bonuses using moving averages.

**Public Interface**:
- **Class `RunningNormalizer`**: Handles running normalization for exploration bonuses.
  - **Key methods**:
    - `__init__(self, shape, epsilon=1e-8, clip_range=(-5, 5), device="cpu")`: Initializes the normalizer with specified parameters.
    - `update(self, x)`: Updates running statistics with new data.
    - `normalize(self, x, clip=True)`: Normalizes input data using current running statistics.
    - `denormalize(self, x)`: Converts normalized data back to the original scale.
    - `state_dict(self)`: Returns the current state of the normalizer for saving.
    - `load_state_dict(self, state_dict)`: Loads the normalizer state from a saved state dictionary.

**Internal Dependencies**:
- **From `torch`**: Utilizes `torch` for tensor operations and device management.
- **External packages**: 
  - `numpy`: Used for numerical operations, though primarily `torch` is used for tensor operations.

**External Dependencies**:
- **Expected to be imported by**: Likely to be used by components handling exploration bonuses, such as `src/rice/exploration/rnd_networks.py`.
- **Key exports used elsewhere**: The `RunningNormalizer` class and its methods for normalizing exploration bonuses.

**Implementation Notes**:
- **Architecture decisions**: The class uses momentum-based updates to maintain running statistics, which helps in stabilizing the normalization process over time.
- **Cross-File Relationships**: This file is part of the exploration module and interacts with other components that calculate or utilize exploration bonuses, potentially integrating with RND networks.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 15 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:01:08
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:  
The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) mechanism, which is used to compute exploration bonuses in reinforcement learning. It includes the architecture for target and predictor networks and manages the normalization of inputs and rewards.

**Public Interface**:  
- Class `RNDNetwork`: Base network architecture for both target and predictor networks.  
  - Key methods: `forward(x)`  
  - Constructor params: `input_dim`, `hidden_dims`, `output_dim`
  
- Class `RNDModule`: Implements the RND mechanism for exploration bonus calculation.  
  - Key methods: `compute_bonus(states, update_stats=True)`, `update(states)`, `state_dict()`, `load_state_dict(state_dict)`  
  - Constructor params: `input_dim`, `hidden_dims`, `output_dim`, `learning_rate`, `device`, `normalize_inputs`

**Internal Dependencies**:  
- From `..exploration.normalizer`: `RunningNormalizer`  
- External packages:  
  - `torch` - Used for neural network operations and tensor computations  
  - `torch.nn` - Provides neural network layers and functionalities  
  - `torch.nn.functional` - Used for loss calculations and activations

**External Dependencies**:  
- Expected to be imported by: Files requiring exploration bonus calculations, likely within the `exploration` and `refinement` modules.  
- Key exports used elsewhere: `RNDModule` class for managing exploration bonuses and network updates.

**Implementation Notes**:  
- Architecture decisions: Utilizes orthogonal initialization for network weights to ensure stable learning. The target network is fixed, while the predictor network is trainable, allowing for the calculation of intrinsic rewards based on the difference between their outputs.
- Cross-File Relationships: Integrates with `RunningNormalizer` for input and reward normalization, facilitating consistent scaling across different states.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 17 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:01:46
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**:  
The `src/rice/exploration/bonus.py` file is responsible for managing the calculation and normalization of exploration bonuses using Random Network Distillation (RND) within the RICE framework. It provides mechanisms to compute, update, and scale these bonuses to enhance exploration in reinforcement learning.

**Public Interface**:  
- **Class `ExplorationBonus`**: Manages exploration bonus calculations using RND networks and normalization.  
  - **Key methods**: 
    - `__init__(state_dim: int, hidden_dims: Tuple[int, ...], output_dim: int, learning_rate: float, bonus_scale: float, normalize_inputs: bool, device: str)`
    - `compute_bonus(states: torch.Tensor, update_stats: bool = True) -> torch.Tensor`
    - `update(states: torch.Tensor) -> Dict[str, float]`
    - `state_dict() -> Dict`
    - `load_state_dict(state_dict: Dict) -> None`
    - `to(device: torch.device) -> 'ExplorationBonus'`

**Internal Dependencies**:  
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - used for tensor operations and device management
  - `numpy` - potentially used for numerical operations

**External Dependencies**:  
- Expected to be imported by: Files that require exploration bonus calculations, likely within the RICE exploration or training modules.
- Key exports used elsewhere: `ExplorationBonus` class and its methods for computing and managing exploration bonuses.

**Implementation Notes**:  
- **Architecture decisions**: The class uses RND to compute exploration bonuses and a running normalizer to scale these bonuses. It supports device management for efficient computation.
- **Cross-File Relationships**: Works closely with `rnd_networks` for RND calculations and `normalizer` for bonus normalization, indicating a modular approach to exploration bonus management.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/refinement/policy.py; ROUND 19 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:02:24
**File Implemented**: src/rice/refinement/policy.py

**Core Purpose**:
- The `src/rice/refinement/policy.py` file implements the Policy Network for the RICE system using a Proximal Policy Optimization (PPO) architecture. It includes both actor (policy) and critic (value) networks to facilitate reinforcement learning tasks.

**Public Interface**:
- **Class `PolicyNetwork`**: Implements a policy network with actor-critic architecture.
  - **Key methods**:
    - `__init__(state_dim: int, action_dim: int, hidden_dims: Tuple[int, ...], activation: nn.Module, log_std_init: float, min_log_std: float, max_log_std: float, device: str)`: Initializes the network with specified dimensions and activation functions.
    - `forward(states: torch.Tensor, deterministic: bool) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`: Computes actions, log probabilities, and value estimates for given states.
    - `evaluate_actions(states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`: Evaluates log probabilities, values, and entropy for given state-action pairs.
    - `get_value(states: torch.Tensor) -> torch.Tensor`: Returns value function estimates for given states.
    - `reset_parameters()`: Initializes network parameters using orthogonal initialization.
    - `to(device: torch.device) -> 'PolicyNetwork'`: Moves the policy network to the specified device.

**Internal Dependencies**:
- From `torch`: `nn`, `nn.functional`, `torch`
- From `typing`: `Tuple`, `Dict`, `Optional`
- External packages: `numpy` - used for numerical operations and initialization.

**External Dependencies**:
- Expected to be imported by: Other components within the RICE system that require policy network functionalities, particularly those involved in training and policy refinement.
- Key exports used elsewhere: `PolicyNetwork` class, which provides the core policy and value estimation functionalities.

**Implementation Notes**:
- Architecture decisions: The network uses a shared feature extractor for both policy and value heads, with configurable hidden layers and activation functions. Log standard deviation is constrained within specified bounds for stability.
- Cross-File Relationships: Likely interacts with training and exploration components to update policies based on reinforcement learning objectives.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/refinement/ppo.py; ROUND 21 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:03:10
**File Implemented**: src/rice/refinement/ppo.py

**Core Purpose**:
- The `src/rice/refinement/ppo.py` file implements the Proximal Policy Optimization (PPO) algorithm tailored for the RICE system. It integrates exploration bonuses into the PPO framework to enhance policy refinement.

**Public Interface**:
- Class `PPO`: Implements the PPO algorithm with clipped objective and combined rewards. 
  - Key methods: `train_step`, `save`, `load`
  - Constructor params: `policy`, `learning_rate`, `n_epochs`, `batch_size`, `clip_range`, `clip_range_vf`, `ent_coef`, `vf_coef`, `max_grad_norm`, `target_kl`, `device`

**Internal Dependencies**:
- From `..refinement.policy`: `PolicyNetwork`
- External packages: 
  - `torch` - Used for tensor operations and model training
  - `torch.nn` - Utilized for neural network components
  - `torch.optim` - Provides optimization algorithms
  - `numpy` - Used for numerical operations

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those managing training loops and policy refinement processes.
- Key exports used elsewhere: The `PPO` class is central to policy refinement and is likely used by training orchestration components.

**Implementation Notes**:
- Architecture decisions: The PPO implementation uses a clipped objective to stabilize training and incorporates exploration bonuses to enhance policy learning.
- Cross-File Relationships: The `PPO` class interacts with the `PolicyNetwork` from `policy.py`, indicating a dependency on policy architecture for action evaluation and updates.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/core/rice.py; ROUND 23 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:03:54
**File Implemented**: src/rice/core/rice.py

**Core Purpose**:
- The `src/rice/core/rice.py` file implements the main RICE system, which integrates explanation-based refinement with exploration techniques to enhance reinforcement learning performance. It orchestrates the interaction between various components such as policy networks, explanation masks, exploration bonuses, and distribution mixing.

**Public Interface**:
- Class `RICE`: Main system for reinforcement learning with integrated critical explanations.
  - Key methods: `__init__`, `update`, `act`, `save`, `load`
  - Constructor params: `state_dim`, `action_dim`, `hidden_dims`, `device`, `mask_hidden_dims`, `alpha`, `beta`, `buffer_size`, `rnd_output_dim`, `rnd_hidden_dims`, `rnd_learning_rate`, `learning_rate`, `n_epochs`, `batch_size`, `clip_range`

**Internal Dependencies**:
- From `..explanation.mask_network`: `StateMask`
- From `..exploration.rnd_networks`: `RNDModule`
- From `..distribution.mixer`: `StateMixer`
- From `..refinement.ppo`: `PPO`
- From `..refinement.policy`: `PolicyNetwork`
- External packages: `torch`, `torch.nn` - used for neural network operations and tensor computations

**External Dependencies**:
- Expected to be imported by: Training and orchestration modules that require the main RICE system functionality
- Key exports used elsewhere: The `RICE` class is central to the system and likely used by components managing training loops and evaluations

**Implementation Notes**:
- Architecture decisions: The RICE system is designed to integrate multiple components seamlessly, leveraging explanation masks and exploration bonuses to refine policy updates.
- Cross-File Relationships: The file coordinates with explanation, exploration, distribution, and refinement modules to implement the complete RICE system.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/core/trainer.py; ROUND 25 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:04:41
**File Implemented**: src/rice/core/trainer.py

**Core Purpose**:  
The `src/rice/core/trainer.py` file is responsible for managing the training loop, evaluation, and logging of results for the RICE (Reinforcement learning with Integrated Critical Explanations) system. It orchestrates the interaction between the environment and the RICE system, handling episodes, evaluations, and model checkpointing.

**Public Interface**:  
- **Class `RICETrainer`**: Manages the training and evaluation of the RICE system.  
  - **Constructor params**: `env_name`, `rice_config`, `max_episodes`, `steps_per_episode`, `eval_frequency`, `num_eval_episodes`, `save_dir`, `device`
  - **Key methods**:
    - `train_episode() -> Tuple[float, int]`: Executes training for one episode and returns the episode reward and length.
    - `evaluate() -> Tuple[float, float]`: Evaluates the current policy and returns the mean and standard deviation of rewards.
    - `train() -> Dict`: Runs the main training loop, returning training statistics.
    - `save_checkpoint(path: str) -> None`: Saves a checkpoint of the RICE system.
    - `load_checkpoint(path: str) -> None`: Loads a checkpoint of the RICE system.

**Internal Dependencies**:  
- From `..core.rice`: `RICE`
- From `..utils.logging`: `Logger`
- External packages: 
  - `os` - for directory operations
  - `torch` - for device management
  - `numpy` - for numerical operations
  - `gym` - for environment management

**External Dependencies**:  
- Expected to be imported by: Other components of the RICE system that require training orchestration.
- Key exports used elsewhere: The `RICETrainer` class is the main export used for training the RICE system.

**Implementation Notes**:  
- **Architecture decisions**: The trainer is designed to handle both training and evaluation, with a focus on modularity and ease of checkpoint management. It uses a logger for tracking training progress.
- **Cross-File Relationships**: The trainer interacts closely with the `RICE` class for decision-making and updates, and utilizes the `Logger` for logging purposes.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/utils/logging.py; ROUND 27 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:05:23
**File Implemented**: src/rice/utils/logging.py

**Core Purpose**: 
- The `src/rice/utils/logging.py` file provides utilities for tracking training metrics and visualizing results within the RICE system. It facilitates logging of training and evaluation metrics, hyperparameters, and model summaries, and supports visualization through plots and TensorBoard.

**Public Interface**:
- Class `Logger`: Manages logging of training and evaluation metrics, hyperparameters, and model summaries.
  - Key methods: 
    - `__init__(log_dir: str, experiment_name: str)`: Initializes the logger with a specified directory and experiment name.
    - `log_episode(episode: int, metrics: Dict[str, Any]) -> None`: Logs metrics for a training episode.
    - `log_evaluation(episode: int, metrics: Dict[str, Any]) -> None`: Logs evaluation metrics.
    - `log_hyperparameters(hparams: Dict[str, Any]) -> None`: Logs hyperparameters for the experiment.
    - `log_model_summary(model: torch.nn.Module) -> None`: Logs the architecture summary of a PyTorch model.
    - `plot_metrics(save: bool = True) -> None`: Plots training metrics and optionally saves them to disk.
    - `save_results() -> None`: Saves all metrics and results to disk.
    - `close() -> None`: Closes the logger and performs cleanup.
    - `get_summary_stats() -> Dict[str, Dict[str, float]]`: Returns summary statistics for all tracked metrics.

**Internal Dependencies**:
- From `os`: `os.makedirs`, `os.path.join`
- From `json`: `json.dump`
- From `time`: `time.time`, `time.strftime`, `time.localtime`
- From `typing`: `Dict`, `Any`, `Optional`, `List`
- From `collections`: `defaultdict`
- From `matplotlib.pyplot`: `plt`
- From `torch.utils.tensorboard`: `SummaryWriter`
- External packages: `numpy` - used for calculating summary statistics of metrics.

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those responsible for training and evaluation processes within the RICE system.
- Key exports used elsewhere: The `Logger` class is the main interface expected to be used by other components for logging purposes.

**Implementation Notes**:
- Architecture decisions: The logger is designed to be flexible, supporting both console output and file-based logging, including JSON serialization for results and metadata.
- Cross-File Relationships: The logger interacts with training components to record metrics and hyperparameters, and it integrates with visualization tools like TensorBoard for real-time monitoring.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/utils/visualization.py; ROUND 29 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:06:24
**File Implemented**: src/rice/utils/visualization.py

**Core Purpose**:
- The `visualization.py` file provides specialized plotting and analysis tools for visualizing the results and progress of the RICE system, including training curves, state masks, exploration analysis, and policy comparisons.

**Public Interface**:
- Class `RICEVisualizer`: Provides visualization tools for analyzing and presenting results from the RICE system.
  - Key methods:
    - `__init__(output_dir: str)`: Initializes the visualizer with a directory for saving plots.
    - `plot_training_curves(metrics: Dict[str, List[float]], title: str = "Training Progress", save_path: Optional[str] = None) -> Figure`: Plots training metrics over time.
    - `plot_state_masks(states: torch.Tensor, masks: torch.Tensor, num_samples: int = 5, save_path: Optional[str] = None) -> Figure`: Visualizes state masks generated by the explanation module.
    - `plot_exploration_analysis(rnd_errors: List[float], visited_states: torch.Tensor, save_path: Optional[str] = None) -> Figure`: Visualizes exploration analysis including RND prediction errors and state coverage.
    - `plot_policy_comparison(original_actions: torch.Tensor, refined_actions: torch.Tensor, states: torch.Tensor, save_path: Optional[str] = None) -> Figure`: Compares actions from original and refined policies.
  - Static method:
    - `_plot_state_vector(ax: Axes, state: np.ndarray, title: str = "") -> None`: Helper method to plot a state vector.

**Internal Dependencies**:
- From `matplotlib`: `Figure`, `Axes`
- External packages:
  - `os`: Used for directory management.
  - `numpy`: Used for numerical operations.
  - `matplotlib.pyplot`: Used for plotting.
  - `seaborn`: Used for enhanced plotting aesthetics.
  - `torch`: Used for tensor operations.
  - `sklearn.decomposition.PCA`: Used for dimensionality reduction in state visualization.

**External Dependencies**:
- Expected to be imported by: Visualization components in the RICE system.
- Key exports used elsewhere: Visualization methods for training progress, state masks, exploration analysis, and policy comparison.

**Implementation Notes**:
- Architecture decisions: Utilizes `matplotlib` and `seaborn` for plotting, with support for saving plots to a specified directory.
- Cross-File Relationships: Likely interacts with components generating metrics, states, masks, and actions for visualization.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 31 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:06:58
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `src/rice/exploration/bonus.py` file is responsible for managing exploration bonuses in reinforcement learning using Random Network Distillation (RND) and normalization techniques. It computes and updates exploration bonuses based on prediction errors from RND networks.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonuses using RND and normalization techniques.
  - Key methods: 
    - `__init__(state_dim: int, rnd_output_dim: int = 128, hidden_dims: Tuple[int, ...] = (256, 256), learning_rate: float = 3e-4, bonus_scale: float = 0.01, momentum: float = 0.99, device: str = "cpu", normalize_inputs: bool = True)`: Initializes the exploration bonus module.
    - `compute_bonus(states: torch.Tensor, update_stats: bool = True) -> torch.Tensor`: Computes normalized exploration bonuses for given states.
    - `update(states: torch.Tensor) -> Dict[str, float]`: Updates the RND predictor network using the given states and returns training metrics.
    - `state_dict() -> Dict`: Returns the state dictionary for saving.
    - `load_state_dict(state_dict: Dict) -> None`: Loads from a state dictionary.

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and device management.
  - `numpy` - General numerical operations.

**External Dependencies**:
- Expected to be imported by: Likely consumer files include components that require exploration bonuses, such as `src/rice/exploration/rnd_networks.py`.
- Key exports used elsewhere: The `ExplorationBonus` class is the main interface expected to be used by other files for managing exploration bonuses.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for exploration bonus calculation and incorporates momentum-based updates for running statistics.
- Cross-File Relationships: Works closely with `rnd_networks` for RND computation and `normalizer` for bonus normalization.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 33 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:07:31
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**: 
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used to enhance exploration in reinforcement learning by providing an intrinsic reward based on the prediction error between a fixed target network and a trainable predictor network.

**Public Interface**:
- Class `RNDModule`: Implements both target and predictor networks for RND.
  - Key methods: `__init__`, `forward`, `get_prediction_error`, `update`, `to`, `state_dict`, `load_state_dict`
  - Constructor params: `state_dim: int`, `hidden_dims: Tuple[int, ...] = (256, 256)`, `output_dim: int = 128`, `device: str = "cpu"`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn as nn`
- External packages: 
  - `torch` - Used for neural network implementation and tensor operations.
  - `typing` - Used for type annotations such as `Tuple` and `Dict`.

**External Dependencies**:
- Expected to be imported by: Files that require RND for exploration, likely within the `exploration` or `core` modules.
- Key exports used elsewhere: `RNDModule` class for implementing exploration strategies in reinforcement learning.

**Implementation Notes**:
- Architecture decisions: The RND module uses a fixed target network and a trainable predictor network, both implemented with fully connected layers. The target network's weights are initialized randomly and remain fixed, while the predictor network is trained to minimize the mean squared error between its output and the target network's output.
- Cross-File Relationships: This module is likely used in conjunction with other exploration strategies and may interact with the `bonus.py` file for calculating exploration bonuses.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 35 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:08:04
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**:
- The `src/rice/exploration/bonus.py` file is responsible for calculating exploration bonuses using Random Network Distillation (RND) prediction errors. It manages the normalization and scaling of these bonuses to ensure stable training within the RICE framework.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND prediction errors.
  - Key methods: 
    - `__init__(state_dim: int, rnd_hidden_dims: Tuple[int, ...], rnd_output_dim: int, bonus_coef: float, normalize_bonus: bool, device: str)`: Initializes the exploration bonus calculator.
    - `compute_bonus(states: torch.Tensor, update_stats: bool) -> torch.Tensor`: Computes exploration bonuses for given states.
    - `update(states: torch.Tensor) -> float`: Updates the RND predictor network using the given states.
    - `to(device: str) -> 'ExplorationBonus'`: Moves the bonus calculator to the specified device.
    - `state_dict() -> dict`: Retrieves the state dictionary for saving.
    - `load_state_dict(state_dict: dict)`: Loads from a state dictionary.

**Internal Dependencies**:
- From `src/rice/exploration`: `RNDModule`, `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and device management.
  - `numpy` - General numerical operations.
  - `typing` - For type hinting with `Optional`, `Union`, `Tuple`.

**External Dependencies**:
- Expected to be imported by: Likely consumer files include other exploration modules or core training modules that require exploration bonuses.
- Key exports used elsewhere: The `ExplorationBonus` class is the main interface expected to be utilized by other components of the RICE system.

**Implementation Notes**:
- Architecture decisions: The class uses RND to compute prediction errors, which are then optionally normalized and scaled to produce the exploration bonus. This approach is designed to enhance exploration by rewarding novelty.
- Cross-File Relationships: This file works closely with `rnd_networks.py` for RND operations and `normalizer.py` for bonus normalization, indicating a modular design where each component handles a specific aspect of the exploration bonus calculation.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 37 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:08:40
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which consists of target and predictor networks used to compute exploration bonuses in reinforcement learning by measuring prediction errors.

**Public Interface**:
- Class `RNDModule`: Implements RND with target and predictor networks.
  - Key methods: `forward(states)`, `get_prediction_error(states)`, `compute_loss(states)`, `state_dict()`, `load_state_dict(state_dict)`, `to(device)`
  - Constructor params: `state_dim`, `hidden_dims`, `output_dim`, `activation`, `device`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn`, `nn.Module`, `nn.Sequential`, `nn.Linear`
- External packages: `numpy` - Used for orthogonal initialization gain calculation

**External Dependencies**:
- Expected to be imported by: `src/rice/exploration/bonus.py`, `src/rice/core/rice.py`
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation

**Implementation Notes**:
- Architecture decisions: Utilizes fixed random weights for the target network and trainable weights for the predictor network to measure prediction error as exploration bonus.
- Cross-File Relationships: Likely interacts with exploration bonus calculation and normalization components to integrate exploration rewards into the reinforcement learning pipeline.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 39 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:09:18
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**:
- The `src/rice/exploration/bonus.py` file is responsible for managing the calculation of exploration bonuses using Random Network Distillation (RND) prediction errors. It includes functionality for normalizing and scaling these bonuses to enhance exploration in reinforcement learning environments.

**Public Interface**:
- **Class `ExplorationBonus`**: Manages exploration bonus calculation using RND prediction errors.
  - **Key methods**:
    - `__init__(rnd_module, bonus_scale: float = 0.01, clip_range: Optional[Tuple[float, float]] = (-5, 5), normalize_bonus: bool = True, device: str = "cpu")`: Initializes the exploration bonus calculator.
    - `update_normalization_stats(bonus_values: torch.Tensor) -> None`: Updates running statistics for bonus normalization.
    - `normalize_bonuses(bonus_values: torch.Tensor) -> torch.Tensor`: Applies normalization to bonus values using running statistics.
    - `compute_bonus(states: torch.Tensor, update_stats: bool = True) -> torch.Tensor`: Computes exploration bonuses for given states.
    - `state_dict() -> dict`: Returns a state dictionary for saving.
    - `load_state_dict(state_dict: dict) -> None`: Loads a state dictionary.
  - **Constructor params**:
    - `rnd_module`: Instance for computing prediction errors.
    - `bonus_scale`: Scaling factor for the exploration bonus.
    - `clip_range`: Optional range for clipping normalized bonuses.
    - `normalize_bonus`: Flag to apply running normalization to bonuses.
    - `device`: Device for computations.

**Internal Dependencies**:
- From `torch`: `Tensor`
- From `numpy`: `np`
- External packages: `torch` and `numpy` are used for tensor operations and numerical computations.

**External Dependencies**:
- Expected to be imported by: Likely consumer files within the `exploration` or `core` modules that require exploration bonus calculations.
- Key exports used elsewhere: The `ExplorationBonus` class is the main interface expected to be used by other components.

**Implementation Notes**:
- **Architecture decisions**: The class uses running statistics to normalize exploration bonuses, which helps in maintaining stable learning dynamics. It also supports optional clipping of bonuses to prevent extreme values.
- **Cross-File Relationships**: This file likely interacts with RND modules for obtaining prediction errors, which are crucial for computing the exploration bonuses.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 41 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:09:54
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:  
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) mechanism, which is used to enhance exploration in reinforcement learning by computing prediction errors between a fixed target network and a trainable predictor network.

**Public Interface**:  
- Class `RNDNetwork`: Base network architecture for target and predictor networks.  
  - Key methods: `forward(x: torch.Tensor) -> torch.Tensor`  
  - Constructor params: `input_dim: int, hidden_dims: Tuple[int, ...], output_dim: int`
  
- Class `RNDModule`: Combines target and predictor networks for RND.  
  - Key methods: `compute_prediction_error(states: torch.Tensor) -> torch.Tensor`, `update(states: torch.Tensor) -> Dict[str, float`, `state_dict() -> Dict`, `load_state_dict(state_dict: Dict) -> None`, `to(device: torch.device) -> 'RNDModule'`  
  - Constructor params: `state_dim: int, hidden_dims: Tuple[int, ...] = (256, 256), output_dim: int = 128, learning_rate: float = 3e-4, device: str = "cpu"`

**Internal Dependencies**:  
- From `torch`: `torch`, `torch.nn`, `torch.nn.functional`  
- External packages: `torch` - used for neural network implementation and optimization

**External Dependencies**:  
- Expected to be imported by: `src/rice/exploration/bonus.py`, `src/rice/core/rice.py`  
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation

**Implementation Notes**:  
- Architecture decisions: Utilizes orthogonal initialization for network weights to ensure stable learning dynamics. The target network is fixed, while the predictor network is trained to minimize the prediction error.
- Cross-File Relationships: Likely interacts with exploration bonus calculation in `bonus.py` to provide novelty detection metrics.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 43 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:10:31
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `src/rice/exploration/bonus.py` file is responsible for managing the calculation and normalization of exploration bonuses using Random Network Distillation (RND) within the RICE framework. It provides a mechanism to compute exploration bonuses that can be used to enhance reinforcement learning agents' exploration strategies.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND.
  - Key methods: `compute_bonus(states, update_stats=True)`, `update(states)`, `to(device)`, `state_dict()`, `load_state_dict(state_dict)`
  - Constructor params: `state_dim`, `hidden_dims`, `output_dim`, `learning_rate`, `bonus_scale`, `device`

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and device management.
  - `numpy` - General numerical operations.

**External Dependencies**:
- Expected to be imported by: Files requiring exploration bonus calculations, likely within the exploration or core modules.
- Key exports used elsewhere: The `ExplorationBonus` class and its methods for computing and updating exploration bonuses.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for exploration bonus calculation, with a focus on normalization and scaling of bonuses. The class is designed to be flexible in terms of device usage (CPU/GPU).
- Cross-File Relationships: Works closely with `rnd_networks.py` for RND computations and `normalizer.py` for bonus normalization.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 45 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:11:03
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used for exploration in reinforcement learning by providing a novelty-based reward signal. It consists of target and predictor networks to compute exploration bonuses.

**Public Interface**:
- Class `MLP`: A multi-layer perceptron network used as a building block for RND networks.
  - Key methods: `forward(x: torch.Tensor) -> torch.Tensor`
  - Constructor params: `input_dim: int, hidden_dims: Tuple[int, ...], output_dim: int`
  
- Class `RNDModule`: Contains the target and predictor networks for RND.
  - Key methods: `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor)`, `compute_loss(states: torch.Tensor) -> torch.Tensor`, `update(states: torch.Tensor) -> Dict[str, float]`, `to(device: torch.device) -> 'RNDModule'`, `state_dict() -> Dict`, `load_state_dict(state_dict: Dict) -> None`
  - Constructor params: `state_dim: int, hidden_dims: Tuple[int, ...] = (256, 256), output_dim: int = 128, learning_rate: float = 1e-3`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn`, `torch.nn.functional`
- External packages: `torch` - used for neural network implementation and optimization

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those handling exploration bonuses and training loops, such as `src/rice/exploration/bonus.py` and `src/rice/core/trainer.py`.
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation and training.

**Implementation Notes**:
- Architecture decisions: Utilizes orthogonal initialization for network weights and employs a fixed random target network to compute novelty-based exploration bonuses.
- Cross-File Relationships: Works in conjunction with exploration bonus calculation and normalization components to integrate exploration rewards into the reinforcement learning pipeline.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 47 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:11:34
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**:
- The `src/rice/exploration/bonus.py` file is responsible for managing the calculation of exploration bonuses in reinforcement learning using Random Network Distillation (RND) and normalization techniques to ensure stable exploration incentives.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND and normalization.
  - Key methods: `compute_bonus`, `update`, `state_dict`, `load_state_dict`
  - Constructor params: `state_dim`, `hidden_dims`, `output_dim`, `learning_rate`, `bonus_scale`, `device`

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and device management
  - `numpy` - General numerical operations
  - `typing` - Type hinting for function parameters and return types

**External Dependencies**:
- Expected to be imported by: Files requiring exploration bonus calculations, likely within the exploration module.
- Key exports used elsewhere: `ExplorationBonus` class for managing exploration bonuses.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for calculating prediction errors and a running normalizer for stabilizing exploration bonuses.
- Cross-File Relationships: Integrates with `rnd_networks` for RND calculations and `normalizer` for bonus normalization, indicating a modular design where exploration components are interconnected.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 49 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:12:10
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used for exploration in reinforcement learning by providing a novelty-based reward signal. It contains both target and predictor networks to compute prediction errors that guide exploration.

**Public Interface**:
- Class `MLP`: A multi-layer perceptron network used for both target and predictor networks.
  - Key methods: `forward(x: torch.Tensor) -> torch.Tensor`
  - Constructor params: `input_dim: int, hidden_dims: Tuple[int, ...], output_dim: int`
  
- Class `RNDModule`: Contains the target and predictor networks for RND.
  - Key methods: `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor)`, `compute_prediction_error(states: torch.Tensor) -> torch.Tensor`, `update(states: torch.Tensor) -> Dict[str, float]`, `to(device: torch.device) -> 'RNDModule'`
  - Constructor params: `state_dim: int, hidden_dims: Tuple[int, ...] = (256, 256), output_dim: int = 128, learning_rate: float = 3e-4`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn`, `torch.optim`
- External packages: `typing` - used for type annotations (`Tuple`, `Dict`)

**External Dependencies**:
- Expected to be imported by: `src/rice/exploration/bonus.py`, `src/rice/core/rice.py`
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation

**Implementation Notes**:
- Architecture decisions: Utilizes orthogonal initialization for network weights to ensure stable learning dynamics. The target network is fixed, while the predictor network is trained to match the target's output.
- Cross-File Relationships: Likely interacts with exploration bonus calculation and core RICE system components to provide exploration signals.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File tests/test_rnd.py; ROUND 51 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:12:36
**File Implemented**: tests/test_rnd.py

**Core Purpose**: 
- The `tests/test_rnd.py` file is responsible for unit testing the Random Network Distillation (RND) module and the exploration bonus calculation within the RICE system. It ensures that the RND module produces outputs of the correct shape and that the exploration bonus is computed correctly.

**Public Interface**:
- Class `TestRNDExploration`: Facilitates unit tests for RND exploration components.
  - Key methods: `setUp()`, `test_rnd_output_shape()`, `test_exploration_bonus()`
  - Constructor params: None (uses `unittest.TestCase` setup)

**Internal Dependencies**:
- From `rice.exploration.rnd_networks`: `RNDModule`
- From `rice.exploration.bonus`: `ExplorationBonus`
- External packages: 
  - `unittest` - Used for structuring and running tests.
  - `torch` - Used for tensor operations and generating random states.

**External Dependencies**:
- Expected to be imported by: Test suite execution scripts or CI/CD pipelines.
- Key exports used elsewhere: None, as this file is primarily for testing purposes.

**Implementation Notes**:
- Architecture decisions: Utilizes PyTorch for tensor operations and random state generation to test the RND module's functionality.
- Cross-File Relationships: Tests the integration between `RNDModule` and `ExplorationBonus` components, ensuring they work together correctly within the exploration framework.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File tests/test_mask.py; ROUND 54 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:13:17
**File Implemented**: tests/test_mask.py

**Core Purpose**:  
The `tests/test_mask.py` file is responsible for unit testing the functionality of the `StateMask` class from the `mask_network.py` module. It ensures that the mask network produces correct output shapes, computes explanation loss accurately, identifies critical states, and maintains mask consistency.

**Public Interface**:  
- Class `TestStateMask`: Unit tests for the `StateMask` class.  
  - Key methods: 
    - `setUp()`: Initializes test parameters and the `StateMask` instance.
    - `test_mask_network_output_shape()`: Tests output shapes and value ranges.
    - `test_explanation_loss()`: Validates explanation loss computation.
    - `test_critical_states_identification()`: Checks critical state identification.
    - `test_mask_consistency()`: Ensures consistency of mask generation.

**Internal Dependencies**:  
- From `rice.explanation.mask_network`: `StateMask`  
- External packages: 
  - `unittest` - Used for structuring and running tests.
  - `torch` - Utilized for tensor operations and neural network functionalities.
  - `numpy` - Used for numerical operations.

**External Dependencies**:  
- Expected to be imported by: Likely consumer files include other test modules or integration tests that require validation of the `StateMask` functionality.
- Key exports used elsewhere: The test results and validation of `StateMask` methods.

**Implementation Notes**:  
- Architecture decisions: The test suite uses `unittest` framework to systematically verify the correctness of the `StateMask` class functionalities.
- Cross-File Relationships: This file tests the `StateMask` class, which is implemented in `mask_network.py`. It ensures that the class behaves as expected and interacts correctly with its inputs and outputs.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/explanation/mask_network.py; ROUND 56 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:13:55
**File Implemented**: src/rice/explanation/mask_network.py

**Core Purpose**:  
The `src/rice/explanation/mask_network.py` file implements the `StateMask` class, which is responsible for generating binary masks for state space exploration and explanation within the RICE algorithm. It aims to enhance the exploration process by identifying critical states and computing explanation losses.

**Public Interface**:  
- **Class `StateMask`**: Implements a neural network to generate state masks for exploration and explanation.  
  - **Key methods**: 
    - `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]`: Performs a forward pass to generate logits, probabilities, and binary actions.
    - `compute_explanation_loss(states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor, alpha: float = 0.0001) -> torch.Tensor`: Computes the explanation loss with a blinding bonus.
    - `get_critical_states(states: torch.Tensor, threshold: float = 0.8) -> torch.Tensor`: Identifies critical states based on mask probabilities.
  - **Constructor params**: `state_dim: int`, `hidden_sizes: List[int] = [128, 128]`, `activation: nn.Module = nn.ReLU`

**Internal Dependencies**:  
- From `torch`: `torch`, `torch.nn`, `torch.nn.functional`  
- External packages: `torch` - used for neural network operations and tensor manipulations.

**External Dependencies**:  
- Expected to be imported by: Likely consumer files include those involved in training and explanation components, such as `src/rice/explanation/training.py`.
- Key exports used elsewhere: The `StateMask` class and its methods are expected to be used for generating masks and computing losses in the RICE system.

**Implementation Notes**:  
- **Architecture decisions**: Utilizes a sequential neural network architecture with customizable hidden layers and activation functions. The network outputs logits for each state dimension, which are converted to probabilities and binary actions.
- **Cross-File Relationships**: Likely interacts with training and explanation modules to provide mask generation and loss computation functionalities.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/explanation/training.py; ROUND 58 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:14:31
**File Implemented**: src/rice/explanation/training.py

**Core Purpose**:
- The `src/rice/explanation/training.py` file is responsible for training the StateMask network using a PPO-style optimization approach. It focuses on updating the mask network to improve explanation fidelity and incorporates a blinding bonus mechanism.

**Public Interface**:
- Class `MaskTrainer`: Facilitates training of the StateMask network with PPO optimization.
  - Key methods: `compute_returns`, `update`, `train_batch`, `save_model`, `load_model`
  - Constructor params: `state_dim`, `learning_rate`, `ppo_epochs`, `batch_size`, `clip_param`, `value_loss_coef`, `entropy_coef`, `max_grad_norm`, `alpha`

**Internal Dependencies**:
- From `rice.explanation.mask_network`: `StateMask`
- External packages: 
  - `torch` - Used for neural network operations and optimizations
  - `numpy` - General numerical operations

**External Dependencies**:
- Expected to be imported by: Likely consumer files include components that require training of the mask network, such as `src/rice/core/trainer.py`.
- Key exports used elsewhere: The `MaskTrainer` class is the main interface for training the mask network, likely used in the core training loop.

**Implementation Notes**:
- Architecture decisions: Utilizes PPO-style optimization with specific hyperparameters for training the mask network, including handling of explanation loss and entropy.
- Cross-File Relationships: Works closely with `StateMask` from `mask_network.py` to apply training updates and explanation objectives.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File setup.py; ROUND 64 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:15:26
**File Implemented**: setup.py

**Core Purpose**:  
The `setup.py` file is responsible for configuring the package distribution for the RICE project. It specifies the package metadata, dependencies, and Python version requirements necessary for installation.

**Public Interface**:  
- Function `setup()`: Configures the package setup for distribution.  
  - Parameters: `name`, `version`, `packages`, `package_dir`, `install_requires`, `python_requires`.  
  - Purpose: Defines the package metadata and dependencies for installation.

**Internal Dependencies**:  
- From `setuptools`: `setup`, `find_packages` - Used for setting up the package configuration and finding all packages within the specified directory.  
- External packages: None directly imported, but specifies dependencies like `torch`, `numpy`, `gym`, `pandas`, `matplotlib`, and `tensorboard` for the package.

**External Dependencies**:  
- Expected to be imported by: No specific files import this; it is used by package managers like `pip` to install the package.  
- Key exports used elsewhere: None, as this file is for configuration purposes only.

**Implementation Notes**:  
- Architecture decisions: The file uses `setuptools` to manage package distribution, which is a standard practice for Python projects. It specifies the source directory as `src` for finding packages.  
- Cross-File Relationships: This file does not directly interact with other files but ensures that all dependencies are available for the entire project.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 66 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:15:56
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**:
- The `src/rice/exploration/bonus.py` file is responsible for calculating and managing exploration bonuses using Random Network Distillation (RND) to provide intrinsic rewards for reinforcement learning agents.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND-based intrinsic rewards.
  - Key methods: `compute_bonus(states, update_rnd=True)`, `update(states)`, `state_dict()`, `load_state_dict(state_dict)`
  - Constructor params: `state_dim`, `hidden_dims`, `output_dim`, `learning_rate`, `device`, `lambda_coef`, `normalize_inputs`

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule` - Used for novelty detection and bonus calculation.
- From `normalizer`: `RunningNormalizer` - Used for normalizing the exploration bonus.
- External packages: `torch` - Used for tensor operations and device management; `numpy` - General numerical operations.

**External Dependencies**:
- Expected to be imported by: Likely consumer files include components that require exploration bonuses, such as policy refinement or training modules.
- Key exports used elsewhere: The `ExplorationBonus` class and its methods are expected to be utilized in other parts of the RICE system for integrating intrinsic rewards.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for intrinsic reward calculation, with a focus on novelty detection and bonus normalization.
- Cross-File Relationships: Works closely with `rnd_networks` for bonus computation and `normalizer` for bonus normalization, indicating a modular design for exploration components.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 68 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:16:30
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:  
The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used to enhance exploration in reinforcement learning by computing a novelty-based exploration bonus. It consists of a target network with fixed random weights and a predictor network that learns to match the target network's outputs.

**Public Interface**:  
- **Class `RNDModule`**: Implements RND with target and predictor networks.  
  - **Key methods**: 
    - `__init__(state_dim: int, hidden_dims: Tuple[int, ...], output_dim: int, learning_rate: float = 3e-4, device: str = "cpu")`: Initializes the RND networks.
    - `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`: Computes outputs from both networks.
    - `compute_prediction_error(states: torch.Tensor) -> torch.Tensor`: Calculates the mean squared error between target and predictor outputs.
    - `update(states: torch.Tensor) -> Dict[str, float]`: Updates the predictor network to minimize the prediction error.
    - `state_dict() -> Dict`: Returns the state dictionary for saving the model.
    - `load_state_dict(state_dict: Dict) -> None`: Loads the model from a state dictionary.
    - `to(device: torch.device) -> 'RNDModule'`: Moves the networks to the specified device.

**Internal Dependencies**:  
- From `torch`: `torch`, `torch.nn`, `torch.nn.functional` - Used for building and training neural networks.
- External packages: `typing` - Used for type annotations.

**External Dependencies**:  
- Expected to be imported by: Files that require RND for exploration, likely within the exploration or core modules.
- Key exports used elsewhere: The `RNDModule` class, providing exploration bonuses based on prediction errors.

**Implementation Notes**:  
- **Architecture decisions**: The RND module uses a fixed target network and a trainable predictor network to compute exploration bonuses, encouraging the agent to explore novel states.
- **Cross-File Relationships**: This module is likely used in conjunction with other exploration strategies and integrated into the main reinforcement learning training loop.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 70 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:17:12
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `src/rice/exploration/bonus.py` file is responsible for calculating and normalizing exploration bonuses using Random Network Distillation (RND) prediction errors. It manages the computation of these bonuses, their normalization, and updates to the RND predictor network.

**Public Interface**:
- **Class `ExplorationBonus`**: Manages exploration bonus calculation using RND prediction errors.
  - **Key methods**:
    - `__init__(state_dim, hidden_dims=(256, 256), output_dim=128, learning_rate=3e-4, bonus_coef=0.01, normalize_bonus=True, device="cpu")`: Initializes the exploration bonus module.
    - `compute_bonus(states, update_stats=True) -> torch.Tensor`: Computes the exploration bonus for given states.
    - `update(states) -> Dict[str, float]`: Updates the RND predictor network.
    - `state_dict() -> Dict`: Retrieves the state dictionary for saving.
    - `load_state_dict(state_dict) -> None`: Loads from a state dictionary.
    - `to(device) -> 'ExplorationBonus'`: Moves the module to the specified device.

**Internal Dependencies**:
- From `src/rice/exploration/normalizer`: `RunningNormalizer`
- From `src/rice/exploration/rnd_networks`: `RNDModule`
- External packages: 
  - `torch` - Used for tensor operations and device management.
  - `numpy` - Used for numerical operations.

**External Dependencies**:
- Expected to be imported by: Likely consumer files in the `exploration` and `core` modules of the RICE implementation.
- Key exports used elsewhere: The `ExplorationBonus` class is expected to be used for managing exploration bonuses in reinforcement learning tasks.

**Implementation Notes**:
- **Architecture decisions**: The class uses a combination of RND for novelty detection and a running normalizer for scaling the exploration bonus.
- **Cross-File Relationships**: The `ExplorationBonus` class relies on `RNDModule` for prediction error calculations and `RunningNormalizer` for bonus normalization, indicating a tightly coupled relationship with these components.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/normalizer.py; ROUND 72 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:17:48
**File Implemented**: src/rice/exploration/normalizer.py

**Core Purpose**:
- The `src/rice/exploration/normalizer.py` file implements a `RunningNormalizer` class that provides running normalization of inputs and rewards to ensure stable training within the RICE framework. It maintains running statistics such as mean and variance to normalize data dynamically.

**Public Interface**:
- Class `RunningNormalizer`: Implements running normalization for input states and rewards.
  - Key methods:
    - `__init__(shape: Union[int, tuple], clip_range: float = 5.0, epsilon: float = 1e-8, device: str = "cpu")`: Initializes the normalizer with specified shape, clipping range, epsilon for numerical stability, and device.
    - `update(x: torch.Tensor) -> None`: Updates running statistics with new data.
    - `normalize(x: torch.Tensor, update_stats: bool = True) -> torch.Tensor`: Normalizes the input tensor using current statistics.
    - `denormalize(x: torch.Tensor) -> torch.Tensor`: Reverses the normalization transform.
    - `state_dict() -> Dict`: Returns the normalizer's state for saving.
    - `load_state_dict(state_dict: Dict) -> None`: Loads the normalizer's state from a dictionary.
    - `to(device: str) -> 'RunningNormalizer'`: Moves the normalizer to the specified device.

**Internal Dependencies**:
- From `torch`: `torch` (used for tensor operations and device management)
- From `numpy`: `numpy` (used for numerical operations)
- External packages: `torch`, `numpy` - used for handling tensor operations and numerical computations.

**External Dependencies**:
- Expected to be imported by: Other components within the RICE framework that require input normalization, such as exploration modules or training loops.
- Key exports used elsewhere: The `RunningNormalizer` class and its methods for maintaining and applying normalization.

**Implementation Notes**:
- Architecture decisions: The class uses Welford's online algorithm for updating mean and variance, ensuring numerical stability and efficiency.
- Cross-File Relationships: Likely interacts with exploration and training components that require normalized inputs for stable learning.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 74 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:18:25
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `bonus.py` file is responsible for managing the exploration bonus calculation using Random Network Distillation (RND) within the RICE framework. It computes normalized intrinsic rewards based on RND prediction errors to encourage exploration in reinforcement learning.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND.
  - Key methods: `__init__()`, `compute_bonus()`, `update()`, `state_dict()`, `load_state_dict()`, `to()`
  - Constructor params: `state_dim`, `hidden_dims`, `output_dim`, `learning_rate`, `bonus_scale`, `normalize_bonus`, `device`

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - for tensor operations and model computations
  - `numpy` - for numerical operations

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those implementing exploration strategies or reinforcement learning training loops.
- Key exports used elsewhere: The `ExplorationBonus` class is the main interface expected to be used by other components in the RICE framework.

**Implementation Notes**:
- Architecture decisions: The class uses RND to compute prediction errors as exploration bonuses, with options for normalization and scaling.
- Cross-File Relationships: Works closely with `rnd_networks.py` for RND computations and `normalizer.py` for bonus normalization.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 76 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:19:07
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**: 
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used to enhance exploration in reinforcement learning by providing intrinsic rewards based on prediction errors between a fixed target network and a trainable predictor network.

**Public Interface**:
- Class `RNDModule`: Implements both target and predictor networks for RND.
  - Key methods: `forward(states)`, `get_prediction_error(states)`, `update(states)`, `state_dict()`, `load_state_dict(state_dict)`
  - Constructor params: `state_dim: int`, `hidden_dims: Tuple[int, ...] = (256, 256)`, `output_dim: int = 128`, `activation: nn.Module = nn.ReLU`, `device: str = "cpu"`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn as nn`, `torch.nn.functional as F`
- External packages: `numpy` (used for weight initialization)

**External Dependencies**:
- Expected to be imported by: `src/rice/exploration/bonus.py`, `src/rice/core/trainer.py`
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation and training

**Implementation Notes**:
- Architecture decisions: Utilizes a fixed target network and a trainable predictor network to compute intrinsic rewards based on prediction errors.
- Cross-File Relationships: Likely interacts with exploration bonus calculation and normalization components to integrate intrinsic rewards into the reinforcement learning pipeline.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 78 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:19:39
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `ExplorationBonus` class in `src/rice/exploration/bonus.py` is responsible for calculating exploration bonuses using Random Network Distillation (RND) prediction errors, which are used to encourage exploration in reinforcement learning environments.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND prediction errors and normalization.
  - Key methods: 
    - `__init__(state_dim, hidden_dims, output_dim, learning_rate, bonus_coef, normalize_states, normalize_bonus, device)`: Initializes the exploration bonus calculator.
    - `preprocess_states(states: torch.Tensor) -> torch.Tensor`: Normalizes states if enabled.
    - `compute_bonus(states: torch.Tensor, update_stats: bool = True) -> torch.Tensor`: Computes exploration bonus for given states.
    - `update(states: torch.Tensor) -> float`: Updates RND predictor network using given states.
    - `state_dict() -> dict`: Returns state dict for saving.
    - `load_state_dict(state_dict: dict)`: Loads from state dict.

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and computations.
  - `numpy` - General numerical operations.
  - `typing` - For type annotations.

**External Dependencies**:
- Expected to be imported by: Files that require exploration bonus calculations, likely within the exploration or training modules.
- Key exports used elsewhere: `ExplorationBonus` class.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for exploration bonus calculation, with options for state and bonus normalization.
- Cross-File Relationships: Relies on `RNDModule` for RND calculations and `RunningNormalizer` for normalization tasks.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 80 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:20:14
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used for exploration in reinforcement learning by computing exploration bonuses through target and predictor networks.

**Public Interface**:
- Class `MLP`: Implements a multi-layer perceptron network used for both target and predictor networks.
  - Key methods: `forward(x: torch.Tensor) -> torch.Tensor`
  - Constructor params: `input_dim: int`, `hidden_dims: Tuple[int, ...]`, `output_dim: int`
  
- Class `RNDModule`: Contains both target and predictor networks for RND exploration.
  - Key methods: `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`, `compute_prediction_error(states: torch.Tensor) -> torch.Tensor`, `update(states: torch.Tensor) -> float`, `state_dict() -> Dict`, `load_state_dict(state_dict: Dict) -> None`, `to(device: torch.device) -> 'RNDModule'`
  - Constructor params: `state_dim: int`, `hidden_dims: Tuple[int, ...]`, `output_dim: int`, `learning_rate: float`, `device: str`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn`, `torch.optim`
- External packages: `torch` - used for neural network implementation and optimization

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those handling exploration bonuses and reinforcement learning training processes.
- Key exports used elsewhere: `RNDModule` class for exploration bonus calculation and predictor network training.

**Implementation Notes**:
- Architecture decisions: Utilizes orthogonal initialization for network weights to ensure stable training and performance. The target network is fixed, and only the predictor network is trained.
- Cross-File Relationships: Works in conjunction with exploration bonus calculation and normalization components to provide exploration incentives in reinforcement learning.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/bonus.py; ROUND 82 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:20:46
**File Implemented**: src/rice/exploration/bonus.py

**Core Purpose**: 
- The `src/rice/exploration/bonus.py` file is responsible for managing the calculation of exploration bonuses using Random Network Distillation (RND) and normalization techniques, as described in the RICE paper. It facilitates the enhancement of exploration strategies in reinforcement learning by providing a mechanism to compute and normalize exploration bonuses.

**Public Interface**:
- Class `ExplorationBonus`: Manages exploration bonus calculation using RND and normalization.
  - Key methods: `compute_bonus(states: torch.Tensor, update_networks: bool = True) -> torch.Tensor`, `update(states: torch.Tensor) -> float`, `state_dict() -> dict`, `load_state_dict(state_dict: dict) -> None`, `to(device: torch.device) -> 'ExplorationBonus'`
  - Constructor params: `state_dim: int`, `hidden_dims: Tuple[int, ...]`, `output_dim: int`, `learning_rate: float`, `bonus_coef: float`, `normalize_bonus: bool`, `device: str`

**Internal Dependencies**:
- From `rnd_networks`: `RNDModule`
- From `normalizer`: `RunningNormalizer`
- External packages: 
  - `torch` - Used for tensor operations and model computations.
  - `numpy` - General numerical operations.
  - `typing` - Type hinting for function signatures.

**External Dependencies**:
- Expected to be imported by: Likely consumer files include those responsible for reinforcement learning training and exploration strategies, such as `src/rice/core/trainer.py` and `src/rice/exploration/rnd_networks.py`.
- Key exports used elsewhere: The `ExplorationBonus` class is the main interface expected to be utilized for computing exploration bonuses in reinforcement learning pipelines.

**Implementation Notes**:
- Architecture decisions: Utilizes RND for exploration bonus calculation, with an option for normalization to ensure stable bonus values. The class design allows for easy integration with reinforcement learning models by providing methods for bonus computation and network updates.
- Cross-File Relationships: Works closely with `rnd_networks` for RND module operations and `normalizer` for bonus normalization, indicating a modular approach to exploration bonus management.

---
*Auto-generated by Memory Agent*



================================================================================
## IMPLEMENTATION File src/rice/exploration/rnd_networks.py; ROUND 84 
================================================================================

# Code Implementation Summary
**Generated**: 2025-08-28 17:21:20
**File Implemented**: src/rice/exploration/rnd_networks.py

**Core Purpose**:
- The `src/rice/exploration/rnd_networks.py` file implements the Random Network Distillation (RND) module, which is used for novelty-based exploration in reinforcement learning. It provides target and predictor networks to compute exploration bonuses based on the difference between their outputs.

**Public Interface**:
- Class `MLP`: A multi-layer perceptron network used for both target and predictor networks.
  - Key methods: `forward(x: torch.Tensor) -> torch.Tensor`
  - Constructor params: `input_dim: int, hidden_dims: Tuple[int, ...], output_dim: int`
  
- Class `RNDModule`: Contains both target and predictor networks for RND.
  - Key methods: `forward(states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`, `compute_loss(states: torch.Tensor) -> torch.Tensor`, `update(states: torch.Tensor) -> float`, `get_exploration_bonus(states: torch.Tensor) -> torch.Tensor`, `state_dict() -> Dict`, `load_state_dict(state_dict: Dict) -> None`
  - Constructor params: `state_dim: int, hidden_dims: Tuple[int, ...], output_dim: int, learning_rate: float, device: str = "cpu"`

**Internal Dependencies**:
- From `torch`: `torch`, `torch.nn as nn`
- External packages: `torch` - used for neural network operations and optimizations.

**External Dependencies**:
- Expected to be imported by: `src/rice/exploration/bonus.py`, `src/rice/core/rice.py`
- Key exports used elsewhere: `RNDModule`, `MLP`

**Implementation Notes**:
- Architecture decisions: The target network is fixed with random weights, while the predictor network is trainable. Orthogonal initialization is used for the weights of the MLP layers.
- Cross-File Relationships: This module is likely used in conjunction with exploration bonus calculations and integrated into the main RICE system for exploration purposes.

---
*Auto-generated by Memory Agent*


